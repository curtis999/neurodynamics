\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{graphicx}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

\input{header.tex}

\title{Criticality in neural networks paper title}


\author{
Paul Rozdeba%\thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.} \\
\\
Department of Physics\\
University of California, San Diego\\
La Jolla, CA 92093 \\
\texttt{prozdeba@physics.ucsd.edu} \\
\And
Forrest Sheldon \\
Department of Physics\\
University of California, San Diego\\
La Jolla, CA 92093 \\
\texttt{forrestemail@physics.ucsd.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

%-------------------------------------------------------------------------------
% ABSTRACT
%-------------------------------------------------------------------------------
\begin{abstract}
It has been posited that biological neural networks, such as a brain, may naturally exist in critical states.  We propose two mechanisms for signal transduction in two such networks as encoding strategies which are optimized by criticality.  First, we examine compressive sensing in a 2-dimensional Ising model at or near its critical temperature.  Secondly, we examine the dynamical synchronization capabilities of a random neural network model as it transitions into chaotic behavior.  We propose that both techniques should be most successful at the critical state of either model.
\end{abstract}


%-------------------------------------------------------------------------------
% INTRODUCTION
%-------------------------------------------------------------------------------
\section{Introduction}


%-------------------------------------------------------------------------------
% PAUL'S SECTION
%-------------------------------------------------------------------------------

\section{Random neural network model}
We considered a neural network model, originally presented by \cite{Sompolinsky1988}, which is a dynamical system describing a network of $N$ randomly connected resitive and capacitive elements.  The equations of motion describing the system are
\begin{align}
	\d{V_i}{t} &= -V_i + \sum_{j=1}^{N} J_{ij} \phi(V_j) \label{eq:m1_eom}
\end{align}
where $\phi(V_i)$ is an $I/V$ relationship which may be thought of as an ``activity'' proportional to the synaptic current between neurons $i$ and $j$.  One possible choice is a sigmoid function of $V$, i.e. $\phi_i = \tanh\left(\alpha V_i\right)$.  This choice is both biologically motivated as acting to saturate synaptic activity as a function of membrane voltage, as well as mathematically to avoid highly unstable, runaway solutions to eq. (\ref{eq:m1_eom}).  $\alpha$ acts as a control parameter on the turnaround rate of the synaptic activity around $V = 0$; in some sense it controls the ``degree of nonlinearity'' in the system.

The matrix $J_{ij}$ describes the connectivity in the network; in this particular model, $J_{ij}$ is chosen to be a Gaussian random matrix with elements distributed according to the statistics
\begin{align}
	\exval{J_{ij}} = 0, \quad \exval{J_{ij}J_{kl}} = \frac{\tilde{J}^2}{N} \: \delta_{ik}\delta_{jl} \label{eq:m1_stats}
\end{align}
with zero on the diagonals to eliminate self-coupling terms.  This means synaptic connections are totally decorrelated and, in general, asymmetric.  This also means that, on average, half of the connections are inherently inhibitory and half are excitatory.  If $J_{ij}$ were symmetric, (\ref{eq:m1_eom}) would describe a system that relaxes to a global minimum of an energy function; with asymmetric couplings the dynamics are that of a spin glass, which in general have nonrelaxational and possibly chaotic behavior.

A detailed mathematical treatment of this model in the limit of large $N$ is given in \cite{Sompolinsky1988}.  The result is that under the replacement $\alpha \rightarrow \tilde{J}$ in the expression for $\phi$, the model undergoes a ``phase transition'' when the control parameter $\tilde{J}$ reaches a critical value of 1.  This is manifest in the structure of the attracting solution manifold in the $\{V_i\}$ state space, which acquires at least one positive Lyapunov exponent; in other words a family of chaotic solutions to (\ref{eq:m1_eom}) appears.

\section{Dynamical synchronization technique}
Dynamical synchronization is a widely-used technique for performing state estimation in systems which can be adequately described by dynamical systems \cite{Abarbanel2009}.  In this technique, the model equations are coupled to a set of data through a linear term as
\begin{align}
	\d{x_i}{t} = f_i(x) + \sum_{j} g_{ij} \left(y_j - x_j\right)
\end{align}
where $x(t)$ and $y(t)$ are the model and the data trajectories, respectively, and $g_{ij}$ is a set of positive coupling constants.  In practice, only a subset of the couplings will be nonzero (corresponding to the components of the system which can be measured) and the matrix $g_{ij}$ is diagonal, since in this scheme there is no obvious advantage to coupling different components of the system to each other\footnote{In a further examination of the model, we hope to test a coupling scheme which couples components to each other in time-delayed coordinates.  Because the components are coupled through the dynamics, the time-delayed trajectories can provide information about each other and enhance the scheme by coupling unmeasurable components to the data.}.  The linear term acts like a driving force on $x$, dragging the coupled components of the dynamics towards the observed values $y$.

It may be possible for \emph{all} the components of a model to synchronize to a data set if enough measurements are made, and if the coupling strenghts are large enough.  What constitutes ``enough'' comes on a case-by-case basis.  Roughly, what is required is for the linear terms to regularize the solution manifold by driving the value of the largest Lyapunov exponent in the coupled system to a non-positive value.

The typical procedure in this kind of analysis is to produce a set of (possibly noisy) dummy data using the model, and couple this back into the model itself.  The coupled ``observer'' model is then integrated forward in time, as usual. This allows one to evaluate whether or not the model can synchronize to a data set which is representative of the dynamics in a controlled setting.  One requires a metric of success to make the call, which might be something as simple as
\begin{align}
	M = \frac{1}{T} \int_0^T dt \; \left(x_i^\text{(obs)}(t) - x_i^\text{(data)}(t)\right)^2
\end{align}
in which case a smaller $M$ means a more successful attempt at synchronization.  This kind of procedure is sometimes referred to as a twin experiment.

\subsection{Numerical analysis}
We examined the model for $N=256$ neurons with a single instantiation of $J_{ij}$.  Ideally, we would like to scale up the simulation size to a larger $N \sim 1000$, and to gather statistics about an \emph{ensemble} of models parameterized by different instantiations of $J_{ij}$.  However, limited by time, we now present said results as at least a preliminary examination of the model.

First, we performed a comparison of numerical results to the analytic results of \cite{Sompolinsky1988}.  For a single randomly drawn instantiation of $J_{ij}$, (\ref{eq:m1_eom}) was integrated forward in time using the \texttt{LSODA} solver in \texttt{scipy.integrate}.  The initial conditions were drawn randomly from a ball of radius 1 centered about the origin $V_i=0$.  To examine the behavior of $\Delta(t)$ alongside the phase space trajectories, it was assumed that the steady state of the system was ergodic so as to allow the replacement
\begin{align*}
	\exval{V_i(t_0)V_i(t+t_0)} \rightarrow \int_{t_0}^{T-t} dt^{\prime} \; V_i(t^{\prime}) V_i(t+t^{\prime})
\end{align*}
(up to an overall scale factor) where $T-t_0$ is the length of the recorded time series.  In other words, the ensemble average was replaced with a time average.  This was necessary because only a single instantiation of $J_{ij}$ was used, and the LHS is an ensemble average over the distribution of $J_{ij}$.  In the analysis, the average value
\begin{align}
	\bar{\Delta}(t) \equiv \frac{1}{N} \sum_i^N \Delta_i(t)
\end{align}
was computed to assess the overall behavior of the system, which for large $N$ should be representative of the behavior of most of the components of the network.

The critical value of $\tilde{J}$ was reckoned to be the value where $\bar{\Delta}$ ``smoothed out'' over $t$.  Below this value, the correlation oscillates with the frequency of the system when it is executing a limit cycle trajectory; well above this value the surface becomes very rough, and peaks sharply at $t=0$.  This is where the solution is chaotic, so time correlations along the trajectory decay quickly.

Finding the critical value of $\tilde{J}$ provided us with a place to examine the effect of criticality on synchronization. The observer system looks like
\begin{align}
	\d{V_i}{t} = -V_i + \sum_{j=1}^{N} J_{ij} \tanh(\tilde{J} V_j) + g_i (V^{(D)}_i - V_i)
\end{align}
where $M$ elements of $g_i$ are nonzero, corresponding to the $M$ measured components of the system (corresponding to the $M$ data time series $V_i^{(D)}$).  This was, again, integrated forward step-by-step over the data points, and compared to the data trajectory.

\subsection{Results}
$\bar{\Delta}(t)$ was calculated for various values of $\tilde{J}$, near to and far from the point where the system becomes chaotic.  The transition appeared to occur at about $\tilde{J} = 1.252$.  Notice this is \emph{not} $\tilde{J}=1$; however, it can be argued that for finite $N$, the solution does not immediately become chaotic but undergoes a period doubling cascade whose width (over $\tilde{J}$) shrinks as $N$ grows (see \cite{Sompolinsky1988}).  In this regard, our numerical result thus shows some agreement with the analytical calculation.

\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/J_1_23}
	\caption{$J=1.23$, phase space trajectory}
	\label{fig:first_pstcorr}
\end{figure}
\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/tcorr_J_1_23}
	\caption{$J=1.23$, time correlation}
\end{figure}
\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/J_1_252}
	\caption{$J=1.252$, phase space trajectory}
\end{figure}
\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/tcorr_J_1_252}
	\caption{$J=1.252$, time correlation}
\end{figure}
\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/J_1_3}
	\caption{$J=1.3$, phase space trajectory}
\end{figure}
\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/tcorr_J_1_3}
	\caption{$J=1.3$, time correlation}
\end{figure}
\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/J_1_4}
	\caption{$J=1.4$, phase space trajectory}
\end{figure}
\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/tcorr_J_1_4}
	\caption{$J=1.4$, time correlation}
	\label{fig:last_pstcorr}
\end{figure}

See figs. \ref{fig:first_pstcorr}-\ref{fig:last_pstcorr} for various comparisons of the phase space trajectory (projection onto two components) to $\bar{\Delta}(t)$.  At $J=1.2$, there is a limit cycle solution; once $J=1.23$, a period-doubling has occurred which is also apparent in $\bar{\Delta}$.

The behavior at $J=1.252$ has already become quite complex.  Notice the structure of the attractor in phase space, as well as that of $\bar{\Delta}$ which has flattened quite a bit by this point. Many individual components of the system, in fact, have become completely flat (not shown).  Beyond this, at $J=1.3$, $\bar{\Delta}$ is showing ``in-between'' behavior where it is oscillating, but also quickly decaying as the model is approaching chaotic behavior.  At $J=1.4$, the model seems to have clearly become chaotic.

\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{paul_figs/sync_J_1_252_g_0_3}
	\caption{Synchronization twin experiment, with data in black and observer in red.}
	\label{fig:twinex}
\end{figure}

It seems like $J=1.252$ is a good place around which to test our hypothesis.  Unfortunately, due to time constraints, we got as far as coding the coupled model and calculating some sample trajectories, but no conclusive results have been obtained yet. See fig. \ref{fig:twinex} for a sample of the outcome at $\tilde{J}=1.252$ and with $N=200$ components of the system coupled to the data.

\subsection{Discussion}
The behavior of $\bar{\Delta}$ is encouraging for the future progress of this project.  It is important that the correlations in many components of the system seem to flatten completely near the critical value of $\tilde{J}$.  This kind of behavior is exactly what is expected out of criticality, where correlation lengths in the system (in both the time and space domains) are expected to diverge.  Ideally, this will produce the effect of maximizing the ability of the system to synchronize near the critical point.

The resulting $\bar{\Delta}$ at other values of $\tilde{J}$ confirm expecations, as well.  On limit cycles, time correlations \emph{should} be periodic since the system is periodic over the period of the cycle.  Additionally, we expect chaotic behavior to quickly degrade correlations and produce very rough, jagged behavior otherwise (which is essentially due to the fractal dimension of the attractor).

Our future direction with this model is to examine its synchronization capabilities both near and far from the critical point, using the aforementioned procedure or, in the future, using time-delayed coordinate information to couple unmeasured components of the model to the data.


%-------------------------------------------------------------------------------
% REFERENCES
%-------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{refs}



\end{document}











